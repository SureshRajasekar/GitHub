{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace Null Value by Mean Median-Towardsdatascience-analyticsvidya-Sigmoid Cuve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
    "import seaborn as sns # used for plot interactive graph. I like it most for plot\n",
    "from sklearn.linear_model import LogisticRegression # to apply the Logistic regression\n",
    "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "from sklearn import metrics # for the check the error and accuracy of the model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# dont worry about the error if its not working then insteda of model_selection we can use cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\",header=0) # here header 0 means the 0 th row is our coloumn name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ANALYSIS AND WRANGLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.head(6))# as u can see our data have imported and having 33 columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can drop this column Unnamed: 32\n",
    "data.drop(\"Unnamed: 32\",axis=1,inplace=True) # in this process this will change in our data itself\n",
    "# if you want to save your old data then you can use below code\n",
    "# data1=data.drop(\"Unnamed:32\",axis=1)\n",
    "# here axis 1 means we are droping the column\n",
    "# here you can check the column has been droped\n",
    "data.columns # this gives the column name which are persent in our data no Unnamed: 32 is not now there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop(\"id\",axis=1,inplace=True) # like this we also don't want the Id column for our analysis\n",
    "# As I said above the data can be divided into three parts.lets divide the features according to their category\n",
    "# now as you know our diagnosis column is a object type so we can map it to integer value\n",
    "data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() # this will describe the all statistical function of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr() # .corr is used for find corelation\n",
    "plt.figure(figsize=(14,14))\n",
    "sns.heatmap(corr, cbar = True,  square = True,\n",
    "            cmap= 'coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the frequency of cancer stages\n",
    "sns.countplot(data['diagnosis'],label=\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'].value_counts()\n",
    "#If data is 100,10000 then it is Rare event Model\n",
    "#Imbalance  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,1:]\n",
    "y=data['diagnosis']\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#testing data size is of 20% of entire data\n",
    "x_train, x_test, y_train, y_test =train_test_split(x,y, test_size = 0.2, random_state =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic=LogisticRegression().fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=logistic.predict(x_test)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Check - Method 1\n",
    "print(logistic.score(x_test,y_test,sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Check - Method 2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,temp)\n",
    "print(metrics.accuracy_score(temp,y_test)) # to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classification_report(y_test,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrics(y_test,temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=10)\n",
    "cross_val_score(clf, x_train, y_train, cv=10)\n",
    "clf.fit(x_train,y_train, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
    "clf.get_params(deep=True)\n",
    "clf.predict(x_test, check_input=True)\n",
    "clf.predict_log_proba(x_test)\n",
    "clf.predict(x_test,check_input=True)\n",
    "print(clf.score(x_test,y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, x_train, y_train, cv=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
